{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCTNeHSAZiww"
   },
   "source": [
    "# Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1214,
     "status": "ok",
     "timestamp": 1610717266561,
     "user": {
      "displayName": "Rizky P. Nugroho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfGfTSVeninJ0B1l8mJI7zRStmfZQEXN4TN6EVgA=s64",
      "userId": "11961087904784672174"
     },
     "user_tz": -420
    },
    "id": "KjxinVdRGayd",
    "outputId": "f2aa1ff0-2a9c-4f1a-8073-82ae5569f967"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/content/feature_data/’: File exists\n",
      "mkdir: cannot create directory ‘/content/models/’: File exists\n"
     ]
    }
   ],
   "source": [
    "# Removed\n",
    "# Competition Use Only\n",
    "\n",
    "!mkdir '/content/feature_data/'\n",
    "!mkdir '/content/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57Lw8krxQyYb"
   },
   "outputs": [],
   "source": [
    "!pip install fasttext -q\n",
    "!pip install fuzzywuzzy -q\n",
    "!pip install textdistance -q\n",
    "!pip install python-Levenshtein -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIm48PpTZoCc"
   },
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KsOLt_eFHE1x"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import joblib\n",
    "import fasttext as ft\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import skew, kurtosis\n",
    "from fuzzywuzzy import fuzz\n",
    "import textdistance\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from lightgbm.sklearn import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bhzhWMuHFS0"
   },
   "outputs": [],
   "source": [
    "PROJECT_DIR = '/content/drive/MyDrive/Colab Projects/product-pair-matching/'\n",
    "DATA_DIR = PROJECT_DIR+'data/'\n",
    "OUTPUTS_DIR = PROJECT_DIR+'outputs/'\n",
    "\n",
    "train_df = pd.read_csv(DATA_DIR+'raw/new_training_set.csv', index_col=0)\n",
    "test_df = pd.read_csv(DATA_DIR+'raw/new_test_set.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35duiRjhCsph"
   },
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNfeqxrtCx4L"
   },
   "outputs": [],
   "source": [
    "default_stop_words = [\n",
    "    'atau', 'dan', 'and', 'murah', 'grosir',\n",
    "    'untuk', 'termurah', 'cod', 'terlaris', 'bisacod', 'terpopuler',\n",
    "    'bisa', 'terbaru', 'tempat', 'populer', 'di', 'sale', 'bayar', 'flash',\n",
    "    'promo', 'seler', 'in', 'salee', 'diskon', 'gila', 'starseller', 'seller'\n",
    "]\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    s = str(text).lower()\n",
    "    s = ' '.join([word for word in s.split() if word not in default_stop_words])\n",
    "    return s\n",
    "\n",
    "def preprocess_text(text):\n",
    "    s = str(text).lower()\n",
    "    s = re.sub('&', ' and ', s)\n",
    "    s = re.sub('/', 'atau', s, count=1)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9]+\", ' ', s)\n",
    "    s = re.sub(' s ', 's ', s)\n",
    "    s = re.sub(r\"([0-9]+(\\.[0-9]+)?)\", r\" \\1 \", s).strip()\n",
    "    return s\n",
    "\n",
    "def preprocess_text_df(df, txt_cols=['title_1', 'title_2'], func=preprocess_text):\n",
    "    txt_df = df[txt_cols].copy()\n",
    "    for col in txt_cols:\n",
    "        txt_df[col] = txt_df.apply(lambda x: func(x[col]), axis=1)\n",
    "    return txt_df\n",
    "    \n",
    "def clean_text(df):\n",
    "    print('Clean text...')\n",
    "    df[['title_1', 'title_2']] = preprocess_text_df(df, txt_cols=['title_1', 'title_2'], \n",
    "                                                    func=preprocess_text)\n",
    "    df[['title_1', 'title_2']] = preprocess_text_df(df, txt_cols=['title_1', 'title_2'], \n",
    "                                                    func=remove_stopwords)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17213,
     "status": "ok",
     "timestamp": 1610717282592,
     "user": {
      "displayName": "Rizky P. Nugroho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfGfTSVeninJ0B1l8mJI7zRStmfZQEXN4TN6EVgA=s64",
      "userId": "11961087904784672174"
     },
     "user_tz": -420
    },
    "id": "NzpXJZICCxwr",
    "outputId": "d64bb690-843a-4b95-c3c8-655573b06877"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "Clean text...\n",
      "TEST\n",
      "Clean text...\n"
     ]
    }
   ],
   "source": [
    "print('TRAIN')\n",
    "train_df = clean_text(train_df)\n",
    "print('TEST')\n",
    "test_df = clean_text(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-N27ihLpYu0b"
   },
   "source": [
    "## Feature Extractor Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjvvWB5ZFz88"
   },
   "source": [
    "### Create text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "executionInfo": {
     "elapsed": 17213,
     "status": "ok",
     "timestamp": 1610717282601,
     "user": {
      "displayName": "Rizky P. Nugroho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfGfTSVeninJ0B1l8mJI7zRStmfZQEXN4TN6EVgA=s64",
      "userId": "11961087904784672174"
     },
     "user_tz": -420
    },
    "id": "gHPWQXJ3Dg3j",
    "outputId": "b209339c-159f-48e7-d8d6-19cdd65f9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concat text...\n",
      "Save text corpus...\n",
      "/content/drive/MyDrive/Colab Projects/product-pair-matching/data/interim/titles.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>johnsons top to toe hair body bath 500 ml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sandal humble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>likuid likuit liquit baby pod liquid salt pod ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6 pasang set anting tusuk bentuk lingkaran aks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rorec natural skin care mask rorec sheet mask ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title\n",
       "0          johnsons top to toe hair body bath 500 ml\n",
       "1                                      sandal humble\n",
       "2  likuid likuit liquit baby pod liquid salt pod ...\n",
       "3  6 pasang set anting tusuk bentuk lingkaran aks...\n",
       "4  rorec natural skin care mask rorec sheet mask ..."
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Concat text...')\n",
    "texts = pd.concat([train_df['title_1'], train_df['title_2'],\n",
    "                   test_df['title_1'], test_df['title_2']], axis=0)\n",
    "texts = texts.reset_index().drop('index', axis=1)\n",
    "texts = texts.rename(columns={0:'Title'})\n",
    "\n",
    "print('Save text corpus...')\n",
    "TXT_DIR = DATA_DIR+'interim/titles.txt'\n",
    "texts.to_csv(TXT_DIR, header=False, index=False)\n",
    "print(TXT_DIR)\n",
    "texts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCU0I1gOF4NB"
   },
   "source": [
    "### Create text model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 92348,
     "status": "ok",
     "timestamp": 1610717357745,
     "user": {
      "displayName": "Rizky P. Nugroho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfGfTSVeninJ0B1l8mJI7zRStmfZQEXN4TN6EVgA=s64",
      "userId": "11961087904784672174"
     },
     "user_tz": -420
    },
    "id": "YVVPjHTKT_i2",
    "outputId": "187dc82f-6308-40c0-f342-dad81f68529b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create text embedding model...\n",
      "Save model...\n",
      "/content/models/fasttext_emb_128.bin\n"
     ]
    }
   ],
   "source": [
    "# TXT_EMB_DIR = OUTPUTS_DIR+'extractor/fasttext_emb_128.bin'\n",
    "TXT_EMB_DIR = '/content/models/fasttext_emb_128.bin'\n",
    "EMB_DIM = 128\n",
    "\n",
    "print('Create text embedding model...')\n",
    "model = ft.train_unsupervised(TXT_DIR, minn=3, maxn=6, dim=EMB_DIM)\n",
    "print('Save model...')\n",
    "model.save_model(TXT_EMB_DIR)\n",
    "print(TXT_EMB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LpSxNe5jZzz0"
   },
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YbtNY_CF9kZ"
   },
   "source": [
    "### Create feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ifi55reUvmZH"
   },
   "outputs": [],
   "source": [
    "def calculate_distance(vect_1, vect_2):\n",
    "    return [\n",
    "        distance.euclidean(vect_1, vect_2),\n",
    "        distance.braycurtis(vect_1, vect_2),\n",
    "        distance.canberra(vect_1, vect_2),\n",
    "        distance.chebyshev(vect_1, vect_2),\n",
    "        distance.cityblock(vect_1, vect_2),\n",
    "        distance.cosine(vect_1, vect_2),\n",
    "        distance.minkowski(vect_1, vect_2),\n",
    "        skew(np.nan_to_num(vect_1)),\n",
    "        skew(np.nan_to_num(vect_2)),\n",
    "        kurtosis(np.nan_to_num(vect_1)),\n",
    "        kurtosis(np.nan_to_num(vect_2)),\n",
    "    ]\n",
    "\n",
    "def calculate_crafted(temp_df):\n",
    "    df = temp_df.copy()\n",
    "    df['len_txt_1'] = df.title_1.apply(lambda x: len(x))\n",
    "    df['len_txt_2'] = df.title_2.apply(lambda x: len(x))\n",
    "    df['len_diff'] = np.abs(df.len_txt_1 - df.len_txt_2)\n",
    "\n",
    "    df['len_char_txt_1'] = df.title_1.apply(lambda x: len(x.replace(' ', '')))\n",
    "    df['len_char_txt_2'] = df.title_2.apply(lambda x: len(x.replace(' ', '')))\n",
    "    df['len_char_diff'] = np.abs(df.len_char_txt_1 - df.len_char_txt_2)\n",
    "\n",
    "    df['len_uniq_char_txt_1'] = df.title_1.apply(lambda x: len(''.join(set(x.replace(' ', '')))))\n",
    "    df['len_uniq_char_txt_2'] = df.title_2.apply(lambda x: len(''.join(set(x.replace(' ', '')))))\n",
    "    df['len_uniq_char_diff'] = np.abs(df.len_uniq_char_txt_1 - df.len_uniq_char_txt_2)\n",
    "\n",
    "    df['len_word_txt_1'] = df.title_1.apply(lambda x: len(x.split()))\n",
    "    df['len_word_txt_2'] = df.title_2.apply(lambda x: len(x.split()))\n",
    "    df['len_word_diff'] = np.abs(df.len_word_txt_1 - df.len_word_txt_2)\n",
    "\n",
    "    df['len_uniq_word_txt_1'] = df.title_1.apply(lambda x: len(set(x.split())))\n",
    "    df['len_uniq_word_txt_2'] = df.title_2.apply(lambda x: len(set(x.split())))\n",
    "    df['len_uniq_word_diff'] = np.abs(df.len_uniq_word_txt_1 - df.len_uniq_word_txt_2)\n",
    "\n",
    "    df['common_words']  = df.apply(lambda x: len(set(x['title_1'].split()).intersection(set(x['title_2'].split()))), axis=1)\n",
    "    df['union_words']   = df.apply(lambda x: len(set(x['title_1'].split()).union(set(x['title_2'].split()))), axis=1)\n",
    "    df['jaccard_words'] = df.common_words / (df.union_words + 1)\n",
    "\n",
    "    df['fuzz_qratio'] = df.apply(lambda x: fuzz.QRatio(x['title_1'], x['title_2']), axis=1)\n",
    "    df['fuzz_WRatio'] = df.apply(lambda x: fuzz.WRatio(x['title_1'], x['title_2']), axis=1)\n",
    "    df['fuzz_partial_ratio'] = df.apply(lambda x: fuzz.partial_ratio(x['title_1'], x['title_2']), axis=1)\n",
    "    df['fuzz_partial_token_set_ratio'] = df.apply(\n",
    "        lambda x: fuzz.partial_token_set_ratio(x['title_1'], x['title_2']), axis=1)\n",
    "    df['fuzz_partial_token_sort_ratio'] = df.apply(\n",
    "        lambda x: fuzz.partial_token_sort_ratio(x['title_1'], x['title_2']), axis=1)\n",
    "    df['fuzz_token_set_ratio'] = df.apply(lambda x: fuzz.token_set_ratio(x['title_1'], x['title_2']), axis=1)\n",
    "    df['fuzz_token_sort_ratio'] = df.apply(lambda x: fuzz.token_sort_ratio(x['title_1'], x['title_2']),\n",
    "                                            axis=1)\n",
    "\n",
    "    df['txt_hamming'] = df.apply(\n",
    "        lambda x: textdistance.hamming.normalized_similarity(x['title_1'], x['title_2']), axis=1)\n",
    "    df['txt_jaro_winkler'] = df.apply(\n",
    "        lambda x: textdistance.jaro_winkler.normalized_similarity(x['title_1'], x['title_2']), axis=1)\n",
    "    df['txt_overlap'] = df.apply(\n",
    "        lambda x: textdistance.overlap.normalized_similarity(x['title_1'], x['title_2']), axis=1)\n",
    "    df['txt_mra'] = df.apply(lambda x: textdistance.mra.normalized_similarity(x['title_1'], x['title_2']),\n",
    "                                axis=1)\n",
    "    df.drop(columns=['title_1', 'title_2'], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KsUtJ78BVnJj"
   },
   "outputs": [],
   "source": [
    "def create_text_feature(temp_df, dir, model=model):\n",
    "    df = temp_df[['title_1', 'title_2']].copy()\n",
    "    print('Calculate text vector...')\n",
    "    vect_1 = []\n",
    "    vect_2 = []\n",
    "    for i, row in df.iterrows():\n",
    "        vect_1.append(model.get_sentence_vector(row['title_1']))\n",
    "        vect_2.append(model.get_sentence_vector(row['title_2']))\n",
    "    \n",
    "    print('Calculate distance...')\n",
    "    distance = []\n",
    "    for i in range(len(vect_1)):\n",
    "        distance.append(calculate_distance(vect_1[i], vect_2[i]))\n",
    "\n",
    "    print('Calculate crafted feature...')\n",
    "    crafted = calculate_crafted(df)\n",
    "    \n",
    "    text_feat = np.concatenate([vect_1, vect_2, distance, crafted.to_numpy()],axis=1)\n",
    "    print('Save text vector...')\n",
    "    joblib.dump(text_feat, dir)\n",
    "    print(dir)\n",
    "    return text_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 226463,
     "status": "ok",
     "timestamp": 1610717491880,
     "user": {
      "displayName": "Rizky P. Nugroho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfGfTSVeninJ0B1l8mJI7zRStmfZQEXN4TN6EVgA=s64",
      "userId": "11961087904784672174"
     },
     "user_tz": -420
    },
    "id": "eZ3NUOij9iXa",
    "outputId": "4ecd6318-9409-4869-edfe-6134d2822e19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN DATA\n",
      "Calculate text vector...\n",
      "Calculate distance...\n",
      "Calculate crafted feature...\n",
      "Save text vector...\n",
      "/content/drive/MyDrive/Colab Projects/product-pair-matching/data/interim/train_text_vector.pkl\n",
      "(10181, 296)\n",
      "TEST DATA\n",
      "Calculate text vector...\n",
      "Calculate distance...\n",
      "Calculate crafted feature...\n",
      "Save text vector...\n",
      "/content/drive/MyDrive/Colab Projects/product-pair-matching/data/interim/test_text_vector.pkl\n",
      "(32580, 296)\n"
     ]
    }
   ],
   "source": [
    "print('TRAIN DATA')\n",
    "TRAIN_VECTOR_DIR = DATA_DIR+'interim/train_text_vector.pkl'\n",
    "train_text_vector = create_text_feature(train_df, TRAIN_VECTOR_DIR)\n",
    "print(train_text_vector.shape)\n",
    "\n",
    "print('TEST DATA')\n",
    "TEST_VECTOR_DIR = DATA_DIR+'interim/test_text_vector.pkl'\n",
    "test_text_vector = create_text_feature(test_df, TEST_VECTOR_DIR)\n",
    "print(test_text_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8-hgckZGJG4"
   },
   "source": [
    "### Create feature dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wOEhDti_KWG_"
   },
   "outputs": [],
   "source": [
    "def create_feature_df(df, vect, dir, label_col=None):\n",
    "    vect_len = 128\n",
    "    dist_len = 11\n",
    "    crafted_len = 29\n",
    "    col_list = [f'txt_1_{i}' for i in range(vect_len)] + \\\n",
    "                [f'txt_2_{i}' for i in range(vect_len)] + \\\n",
    "                [f'txt_dist_{i}' for i in range(dist_len)] + \\\n",
    "                [f'txt_crafted_{i}' for i in range(crafted_len)]\n",
    "\n",
    "    feats_df = pd.DataFrame(\n",
    "        data=vect,\n",
    "        columns=col_list)\n",
    "    \n",
    "    if label_col is not None:\n",
    "        feats_df[label_col] = df[label_col]\n",
    "\n",
    "    print('Save dataframe...')\n",
    "    feats_df.to_csv(dir, index=False)\n",
    "    print(dir)\n",
    "    return feats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 248561,
     "status": "ok",
     "timestamp": 1610717513991,
     "user": {
      "displayName": "Rizky P. Nugroho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfGfTSVeninJ0B1l8mJI7zRStmfZQEXN4TN6EVgA=s64",
      "userId": "11961087904784672174"
     },
     "user_tz": -420
    },
    "id": "39Fc12ZY_jAa",
    "outputId": "443d2930-72ac-409d-eb27-f5ed86691419"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN DATA\n",
      "Save dataframe...\n",
      "/content/drive/MyDrive/Colab Projects/product-pair-matching/data/clean/train_text_df.csv\n",
      "(10181, 297)\n",
      "TEST DATA\n",
      "Save dataframe...\n",
      "/content/drive/MyDrive/Colab Projects/product-pair-matching/data/clean/test_text_df.csv\n",
      "(32580, 296)\n"
     ]
    }
   ],
   "source": [
    "print('TRAIN DATA')\n",
    "TRAIN_DF_DIR = DATA_DIR+'clean/train_text_df.csv'\n",
    "train_text_df = create_feature_df(train_df, train_text_vector, TRAIN_DF_DIR, 'Label')\n",
    "print(train_text_df.shape)\n",
    "\n",
    "print('TEST DATA')\n",
    "TEST_DF_DIR = DATA_DIR+'clean/test_text_df.csv'\n",
    "test_text_df = create_feature_df(test_df, test_text_vector, TEST_DF_DIR)\n",
    "print(test_text_df.shape)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNXL5P4BquM4+lGnCPk8xoP",
   "collapsed_sections": [],
   "mount_file_id": "18a3ZpMVTiIe49oKr5To3rYnid5CVxGtA",
   "name": "text_feature_extraction.ipynb",
   "provenance": [
    {
     "file_id": "14g5OtqrcnvyIvyGXZ_8Vp2ci4POVqkoO",
     "timestamp": 1610536464729
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
